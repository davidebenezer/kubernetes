* Create 3 medium with Ubuntu Bionic 18.4

Step 1: Do on all master
========================
			sudo hostnamectl set-hostname k8s-master
		Do on worker1
			sudo hostnamectl set-hostname k8s-worker1
		Do on worker2
			sudo hostnamectl set-hostname k8s-worker2
		
Step 2: since host name is changed. update the /etc/hosts in all servers
========================================================================

172.31.42.90 k8s-master
172.31.47.197 k8s-worker1
172.31.47.243 k8s-worker2
		

Step3: Install containerd runtime in all 3 servers
==================================================

First enable kerner modules
---------------------------
cat << EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br-netfilter
EOF

Below step to make this change available immediately instead of server restart
------------------------------------------------------------------------------
sudo modprobe overlay
sudo modprobe br-netfilter

Additional config needed for containerd and kubernetes for networking
---------------------------------------------------------------------
cat << EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-ip-tables = 1
net.ipv4.ip_forward =1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

Make changes available instantaneously instead of restart
---------------------------------------------------------
sudo sysctl --system

Install Containerd
------------------
sudo apt-get update && sudo apt-get install -y containerd

Default Configuration for containerd
------------------------------------
sudo mkdir -p /etc/containerd

	generate default config and save it in a file
	---------------------------------------------
sudo containerd config default | sudo tee /etc/containerd/config.toml 	

	config to take effect immediately
	---------------------------------
sudo systemctl restart containerd

Disable Swap for kubernetes to work
-----------------------------------
sudo swapoff -a

Add some setting to disable swap in /etc/fstab
----------------------------------------------
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

Install required packages listed in kubernetes documentation
------------------------------------------------------------
sudo apt-get update && sudo apt-get install -y apt-transport-https curl

Add Gpg key for kubernetes package repository - download and add the key
------------------------------------------------------------------------
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

Add Kubernetes Package Repository itself
----------------------------------------
cat << EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF

Update Packages
---------------
sudo apt-get update

Install Kubernetes Packages
---------------------------
sudo apt-get install -y kubelet=1.22.0-00 kubeadm=1.22.0-00 kubectl=1.22.0-00

Prevent packages in hold to prevent automatic updates
-----------------------------------------------------
sudo apt-mark hold kubelet kubeadm kubectl


Step 4: Initialize cluster only in Control Pane Node pod network cidr added to be used later on
==============================================================================================
sudo kubeadm init --pod-network-cidr 192.168.0.0/16 --kubernetes-version 1.22.0

set kubeconfig from output	
--------------------------
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

check nodes
-----------
kubectl get nodes

Configure Calico Networking to make it ready(link in video not working. Use below)
----------------------------------------------------------------------------------
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml

----only run if previous step of calico.yml doesnot work:## kubectl apply -f https://projectcalico.docs.tigera.io/manifests/calico.yaml

Join the worked nodes. Get info from control pane
-------------------------------------------------
kubeadm token create --print-join-command


Step 5: Run the token command generated in each of worker nodes(run as root)
============================================================================
sudo kubeadm join 172.31.42.90:6443 --token sr2dzy.2lldz8v8m1mmtjkt --discovery-token-ca-cert-hash sha256:ab5341d56ff5460be7716f8f648e9ddc6a457db968b244fc4fb81a9f8c271421

All nodes will be ready
-----------------------
kubectl get nodes